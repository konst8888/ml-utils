{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import datetime as dt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstantinlipkin/anaconda3/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6188914, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>event_time</th>\n",
       "      <th>action</th>\n",
       "      <th>theta</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>2020-09-21 20:57:06.025613</td>\n",
       "      <td>2020-09-14 18:56:54.933907</td>\n",
       "      <td>session</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>2020-09-21 20:57:06.025613</td>\n",
       "      <td>2020-09-14 18:56:59.276285</td>\n",
       "      <td>paid_feed_view</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>2020-09-21 20:57:06.025613</td>\n",
       "      <td>2020-09-14 18:56:59.276285</td>\n",
       "      <td>paid_feed_view</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>2020-09-21 20:57:06.025613</td>\n",
       "      <td>2020-09-14 18:56:59.276285</td>\n",
       "      <td>paid_feed_view</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>2020-09-21 20:57:06.025613</td>\n",
       "      <td>2020-09-14 18:57:00.782144</td>\n",
       "      <td>non_paid_feed_view</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                  created_at                  event_time  \\\n",
       "0       14  2020-09-21 20:57:06.025613  2020-09-14 18:56:54.933907   \n",
       "1       14  2020-09-21 20:57:06.025613  2020-09-14 18:56:59.276285   \n",
       "2       14  2020-09-21 20:57:06.025613  2020-09-14 18:56:59.276285   \n",
       "3       14  2020-09-21 20:57:06.025613  2020-09-14 18:56:59.276285   \n",
       "4       14  2020-09-21 20:57:06.025613  2020-09-14 18:57:00.782144   \n",
       "\n",
       "               action    theta  label  \n",
       "0             session      NaN    0.0  \n",
       "1      paid_feed_view  22000.0    0.0  \n",
       "2      paid_feed_view  19000.0    0.0  \n",
       "3      paid_feed_view      0.0    0.0  \n",
       "4  non_paid_feed_view      NaN    0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('actions.csv', index_col=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['user_id', 'event_time', 'action']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12564it [00:01, 8774.35it/s]\n"
     ]
    }
   ],
   "source": [
    "idxs_drop = []\n",
    "user_list = []\n",
    "for idx, row in tqdm(df[df.action == 'referral_made'].iterrows()):\n",
    "    user_id = row.user_id\n",
    "    if user_id not in user_list:\n",
    "        user_list.append(user_id)\n",
    "    else:\n",
    "        idxs_drop.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6180271, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[~df.index.isin(idxs_drop)]\n",
    "del idxs_drop\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['user_id', 'event_time'], inplace=True);\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Actions:\n",
    "    def __init__(self):\n",
    "        self.action2index = {}\n",
    "        self.action2count = {}\n",
    "        self.index2action = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_actions = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSequence(self, sentence):\n",
    "        for action in sentence:\n",
    "            self.addAction(action)\n",
    "\n",
    "    def addAction(self, action):\n",
    "        if action not in self.action2index:\n",
    "            self.action2index[action] = self.n_actions\n",
    "            self.action2count[action] = 1\n",
    "            self.index2action[self.n_actions] = action\n",
    "            self.n_actions += 1\n",
    "        else:\n",
    "            self.action2count[action] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences():\n",
    "    df_dict = df.to_dict()\n",
    "\n",
    "    user_idxs = {}\n",
    "    for k, v in df_dict['user_id'].items():\n",
    "        user_idxs[v] = user_idxs.get(v, []) + [k]\n",
    "\n",
    "    sequences = dict()\n",
    "\n",
    "    for user_id, idxs in tqdm(user_idxs.items()):\n",
    "        actions = [df_dict['action'][idx] for idx in idxs]\n",
    "        taus = [df_dict['event_time'][idx] for idx in idxs]\n",
    "        taus = [dt.datetime.timestamp(dt.datetime.strptime(t, '%Y-%m-%d %H:%M:%S.%f')) for t in taus]\n",
    "        taus = [t / max(taus) for t in taus]\n",
    "        sequences[user_id] = list(zip(actions, taus))\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108646/108646 [02:21<00:00, 766.40it/s]\n"
     ]
    }
   ],
   "source": [
    "sequences = get_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 23569),\n",
       " (2, 6988),\n",
       " (3, 3283),\n",
       " (4, 2165),\n",
       " (5, 2110),\n",
       " (6, 1733),\n",
       " (7, 1584),\n",
       " (8, 1534),\n",
       " (9, 1387),\n",
       " (11, 1338),\n",
       " (10, 1315),\n",
       " (12, 1246),\n",
       " (13, 1185),\n",
       " (14, 1136),\n",
       " (15, 1132),\n",
       " (17, 1055),\n",
       " (18, 1016),\n",
       " (19, 1009),\n",
       " (16, 1001),\n",
       " (20, 949),\n",
       " (22, 890),\n",
       " (23, 869),\n",
       " (24, 865),\n",
       " (21, 860),\n",
       " (25, 824),\n",
       " (27, 823),\n",
       " (26, 809),\n",
       " (32, 764),\n",
       " (29, 759),\n",
       " (28, 759),\n",
       " (31, 728),\n",
       " (30, 710),\n",
       " (33, 700),\n",
       " (34, 662),\n",
       " (38, 651),\n",
       " (37, 643),\n",
       " (36, 613),\n",
       " (35, 603),\n",
       " (39, 580),\n",
       " (41, 578),\n",
       " (43, 550),\n",
       " (40, 537),\n",
       " (42, 531),\n",
       " (46, 522),\n",
       " (44, 511),\n",
       " (47, 500),\n",
       " (45, 485),\n",
       " (51, 470),\n",
       " (48, 461),\n",
       " (52, 458),\n",
       " (49, 445),\n",
       " (54, 435),\n",
       " (53, 435),\n",
       " (55, 435),\n",
       " (50, 420),\n",
       " (57, 416),\n",
       " (56, 396),\n",
       " (59, 382),\n",
       " (62, 382),\n",
       " (63, 362),\n",
       " (58, 359),\n",
       " (65, 358),\n",
       " (61, 355),\n",
       " (64, 348),\n",
       " (60, 345),\n",
       " (66, 340),\n",
       " (67, 328),\n",
       " (72, 320),\n",
       " (69, 320),\n",
       " (76, 316),\n",
       " (70, 309),\n",
       " (73, 299),\n",
       " (68, 298),\n",
       " (74, 297),\n",
       " (71, 288),\n",
       " (75, 278),\n",
       " (78, 263),\n",
       " (79, 259),\n",
       " (77, 257),\n",
       " (84, 253),\n",
       " (85, 248),\n",
       " (82, 248),\n",
       " (92, 244),\n",
       " (83, 239),\n",
       " (88, 238),\n",
       " (80, 234),\n",
       " (86, 230),\n",
       " (81, 230),\n",
       " (87, 227),\n",
       " (100, 226),\n",
       " (93, 225),\n",
       " (91, 223),\n",
       " (90, 220),\n",
       " (103, 216),\n",
       " (99, 204),\n",
       " (94, 204),\n",
       " (96, 203),\n",
       " (89, 202),\n",
       " (98, 202),\n",
       " (97, 201),\n",
       " (95, 199),\n",
       " (102, 186),\n",
       " (107, 186),\n",
       " (104, 185),\n",
       " (114, 181),\n",
       " (101, 179),\n",
       " (106, 178),\n",
       " (112, 172),\n",
       " (111, 171),\n",
       " (105, 168),\n",
       " (113, 166),\n",
       " (108, 163),\n",
       " (110, 160),\n",
       " (121, 156),\n",
       " (122, 154),\n",
       " (109, 151),\n",
       " (116, 148),\n",
       " (118, 147),\n",
       " (131, 144),\n",
       " (117, 141),\n",
       " (120, 141),\n",
       " (123, 135),\n",
       " (125, 134),\n",
       " (115, 134),\n",
       " (127, 131),\n",
       " (130, 129),\n",
       " (129, 129),\n",
       " (136, 129),\n",
       " (135, 128),\n",
       " (128, 124),\n",
       " (126, 124),\n",
       " (119, 123),\n",
       " (137, 123),\n",
       " (145, 121),\n",
       " (149, 120),\n",
       " (134, 118),\n",
       " (139, 118),\n",
       " (133, 118),\n",
       " (144, 117),\n",
       " (140, 115),\n",
       " (124, 112),\n",
       " (138, 112),\n",
       " (132, 107),\n",
       " (142, 105),\n",
       " (143, 105),\n",
       " (150, 103),\n",
       " (157, 102),\n",
       " (141, 101),\n",
       " (163, 100),\n",
       " (154, 99),\n",
       " (147, 99),\n",
       " (146, 96),\n",
       " (148, 95),\n",
       " (164, 94),\n",
       " (156, 94),\n",
       " (153, 92),\n",
       " (162, 91),\n",
       " (158, 90),\n",
       " (151, 89),\n",
       " (166, 87),\n",
       " (167, 85),\n",
       " (171, 83),\n",
       " (152, 82),\n",
       " (177, 82),\n",
       " (183, 80),\n",
       " (168, 78),\n",
       " (159, 78),\n",
       " (175, 77),\n",
       " (161, 76),\n",
       " (155, 76),\n",
       " (165, 75),\n",
       " (160, 75),\n",
       " (176, 74),\n",
       " (170, 74),\n",
       " (187, 73),\n",
       " (188, 73),\n",
       " (181, 70),\n",
       " (185, 70),\n",
       " (172, 69),\n",
       " (204, 68),\n",
       " (193, 66),\n",
       " (186, 66),\n",
       " (169, 65),\n",
       " (173, 64),\n",
       " (192, 63),\n",
       " (178, 63),\n",
       " (190, 61),\n",
       " (182, 61),\n",
       " (180, 60),\n",
       " (184, 59),\n",
       " (179, 58),\n",
       " (222, 57),\n",
       " (196, 57),\n",
       " (210, 56),\n",
       " (191, 56),\n",
       " (195, 55),\n",
       " (174, 54),\n",
       " (189, 54),\n",
       " (199, 53),\n",
       " (197, 53),\n",
       " (212, 53),\n",
       " (218, 52),\n",
       " (200, 50),\n",
       " (205, 50),\n",
       " (209, 50),\n",
       " (219, 50),\n",
       " (211, 50),\n",
       " (207, 49),\n",
       " (194, 49),\n",
       " (208, 48),\n",
       " (213, 48),\n",
       " (229, 48),\n",
       " (233, 47),\n",
       " (236, 47),\n",
       " (217, 46),\n",
       " (203, 46),\n",
       " (249, 46),\n",
       " (198, 46),\n",
       " (214, 45),\n",
       " (221, 45),\n",
       " (215, 44),\n",
       " (206, 44),\n",
       " (231, 44),\n",
       " (245, 43),\n",
       " (224, 43),\n",
       " (216, 43),\n",
       " (230, 42),\n",
       " (235, 42),\n",
       " (225, 41),\n",
       " (240, 41),\n",
       " (246, 40),\n",
       " (202, 40),\n",
       " (201, 40),\n",
       " (258, 40),\n",
       " (252, 40),\n",
       " (270, 40),\n",
       " (239, 39),\n",
       " (241, 38),\n",
       " (232, 37),\n",
       " (223, 37),\n",
       " (243, 36),\n",
       " (257, 36),\n",
       " (262, 36),\n",
       " (234, 35),\n",
       " (275, 35),\n",
       " (228, 35),\n",
       " (250, 34),\n",
       " (259, 34),\n",
       " (254, 34),\n",
       " (248, 33),\n",
       " (244, 33),\n",
       " (263, 32),\n",
       " (264, 32),\n",
       " (226, 32),\n",
       " (242, 32),\n",
       " (247, 31),\n",
       " (261, 31),\n",
       " (251, 31),\n",
       " (266, 31),\n",
       " (220, 31),\n",
       " (238, 31),\n",
       " (268, 31),\n",
       " (274, 30),\n",
       " (227, 29),\n",
       " (279, 29),\n",
       " (260, 29),\n",
       " (276, 28),\n",
       " (237, 28),\n",
       " (327, 27),\n",
       " (253, 27),\n",
       " (280, 27),\n",
       " (299, 26),\n",
       " (321, 26),\n",
       " (281, 26),\n",
       " (256, 25),\n",
       " (271, 25),\n",
       " (309, 25),\n",
       " (303, 24),\n",
       " (278, 24),\n",
       " (313, 24),\n",
       " (286, 24),\n",
       " (267, 24),\n",
       " (295, 24),\n",
       " (305, 24),\n",
       " (318, 24),\n",
       " (272, 24),\n",
       " (298, 23),\n",
       " (269, 23),\n",
       " (304, 23),\n",
       " (288, 23),\n",
       " (314, 22),\n",
       " (284, 22),\n",
       " (336, 22),\n",
       " (310, 22),\n",
       " (273, 22),\n",
       " (292, 22),\n",
       " (282, 22),\n",
       " (302, 22),\n",
       " (334, 22),\n",
       " (346, 22),\n",
       " (326, 21),\n",
       " (312, 21),\n",
       " (265, 21),\n",
       " (255, 21),\n",
       " (277, 20),\n",
       " (308, 20),\n",
       " (354, 20),\n",
       " (301, 20),\n",
       " (333, 20),\n",
       " (293, 20),\n",
       " (323, 19),\n",
       " (296, 19),\n",
       " (332, 19),\n",
       " (335, 19),\n",
       " (285, 19),\n",
       " (311, 19),\n",
       " (359, 19),\n",
       " (342, 19),\n",
       " (355, 19),\n",
       " (369, 19),\n",
       " (283, 18),\n",
       " (289, 18),\n",
       " (287, 18),\n",
       " (331, 18),\n",
       " (328, 18),\n",
       " (315, 18),\n",
       " (338, 18),\n",
       " (291, 18),\n",
       " (307, 17),\n",
       " (297, 17),\n",
       " (329, 17),\n",
       " (356, 17),\n",
       " (379, 17),\n",
       " (300, 17),\n",
       " (349, 17),\n",
       " (294, 17),\n",
       " (357, 17),\n",
       " (319, 16),\n",
       " (381, 16),\n",
       " (383, 16),\n",
       " (361, 16),\n",
       " (340, 16),\n",
       " (306, 16),\n",
       " (367, 16),\n",
       " (380, 16),\n",
       " (337, 16),\n",
       " (410, 16),\n",
       " (362, 16),\n",
       " (370, 16),\n",
       " (415, 15),\n",
       " (339, 15),\n",
       " (345, 15),\n",
       " (324, 15),\n",
       " (360, 15),\n",
       " (347, 15),\n",
       " (351, 15),\n",
       " (515, 14),\n",
       " (322, 14),\n",
       " (423, 14),\n",
       " (455, 14),\n",
       " (365, 14),\n",
       " (290, 14),\n",
       " (316, 14),\n",
       " (320, 14),\n",
       " (325, 14),\n",
       " (341, 13),\n",
       " (317, 13),\n",
       " (343, 13),\n",
       " (350, 13),\n",
       " (424, 13),\n",
       " (375, 13),\n",
       " (372, 13),\n",
       " (483, 13),\n",
       " (348, 13),\n",
       " (377, 12),\n",
       " (378, 12),\n",
       " (384, 12),\n",
       " (392, 12),\n",
       " (376, 12),\n",
       " (437, 12),\n",
       " (404, 12),\n",
       " (441, 12),\n",
       " (418, 12),\n",
       " (382, 11),\n",
       " (373, 11),\n",
       " (480, 11),\n",
       " (403, 11),\n",
       " (388, 11),\n",
       " (487, 11),\n",
       " (352, 11),\n",
       " (412, 11),\n",
       " (429, 11),\n",
       " (389, 11),\n",
       " (438, 11),\n",
       " (405, 11),\n",
       " (353, 11),\n",
       " (432, 11),\n",
       " (408, 11),\n",
       " (344, 11),\n",
       " (439, 11),\n",
       " (407, 11),\n",
       " (478, 10),\n",
       " (386, 10),\n",
       " (396, 10),\n",
       " (385, 10),\n",
       " (393, 10),\n",
       " (421, 10),\n",
       " (411, 10),\n",
       " (371, 10),\n",
       " (416, 10),\n",
       " (330, 10),\n",
       " (548, 10),\n",
       " (395, 10),\n",
       " (399, 10),\n",
       " (409, 9),\n",
       " (471, 9),\n",
       " (530, 9),\n",
       " (401, 9),\n",
       " (433, 9),\n",
       " (398, 9),\n",
       " (482, 9),\n",
       " (538, 9),\n",
       " (458, 9),\n",
       " (450, 9),\n",
       " (445, 9),\n",
       " (400, 9),\n",
       " (479, 9),\n",
       " (434, 9),\n",
       " (460, 9),\n",
       " (364, 9),\n",
       " (422, 8),\n",
       " (394, 8),\n",
       " (599, 8),\n",
       " (525, 8),\n",
       " (509, 8),\n",
       " (390, 8),\n",
       " (472, 8),\n",
       " (413, 8),\n",
       " (374, 8),\n",
       " (366, 8),\n",
       " (550, 8),\n",
       " (443, 8),\n",
       " (446, 8),\n",
       " (558, 8),\n",
       " (492, 8),\n",
       " (368, 8),\n",
       " (397, 8),\n",
       " (465, 8),\n",
       " (457, 8),\n",
       " (391, 8),\n",
       " (497, 8),\n",
       " (495, 8),\n",
       " (419, 8),\n",
       " (461, 8),\n",
       " (358, 8),\n",
       " (553, 8),\n",
       " (406, 7),\n",
       " (499, 7),\n",
       " (444, 7),\n",
       " (425, 7),\n",
       " (427, 7),\n",
       " (466, 7),\n",
       " (431, 7),\n",
       " (467, 7),\n",
       " (402, 7),\n",
       " (417, 7),\n",
       " (503, 7),\n",
       " (414, 7),\n",
       " (387, 7),\n",
       " (430, 7),\n",
       " (447, 7),\n",
       " (516, 7),\n",
       " (436, 7),\n",
       " (569, 7),\n",
       " (475, 7),\n",
       " (506, 7),\n",
       " (563, 7),\n",
       " (426, 7),\n",
       " (448, 7),\n",
       " (456, 7),\n",
       " (597, 6),\n",
       " (611, 6),\n",
       " (363, 6),\n",
       " (610, 6),\n",
       " (474, 6),\n",
       " (420, 6),\n",
       " (501, 6),\n",
       " (539, 6),\n",
       " (531, 6),\n",
       " (459, 6),\n",
       " (722, 6),\n",
       " (493, 6),\n",
       " (705, 6),\n",
       " (476, 6),\n",
       " (490, 6),\n",
       " (600, 6),\n",
       " (485, 6),\n",
       " (473, 6),\n",
       " (451, 6),\n",
       " (625, 6),\n",
       " (518, 5),\n",
       " (657, 5),\n",
       " (442, 5),\n",
       " (453, 5),\n",
       " (489, 5),\n",
       " (655, 5),\n",
       " (524, 5),\n",
       " (585, 5),\n",
       " (513, 5),\n",
       " (582, 5),\n",
       " (541, 5),\n",
       " (647, 5),\n",
       " (440, 5),\n",
       " (684, 5),\n",
       " (428, 5),\n",
       " (523, 5),\n",
       " (580, 5),\n",
       " (555, 5),\n",
       " (535, 5),\n",
       " (603, 5),\n",
       " (564, 5),\n",
       " (508, 5),\n",
       " (552, 5),\n",
       " (452, 5),\n",
       " (498, 5),\n",
       " (488, 5),\n",
       " (449, 5),\n",
       " (568, 5),\n",
       " (470, 5),\n",
       " (532, 5),\n",
       " (463, 5),\n",
       " (674, 5),\n",
       " (725, 5),\n",
       " (542, 4),\n",
       " (698, 4),\n",
       " (598, 4),\n",
       " (754, 4),\n",
       " (685, 4),\n",
       " (561, 4),\n",
       " (791, 4),\n",
       " (547, 4),\n",
       " (574, 4),\n",
       " (468, 4),\n",
       " (494, 4),\n",
       " (562, 4),\n",
       " (588, 4),\n",
       " (628, 4),\n",
       " (968, 4),\n",
       " (522, 4),\n",
       " (823, 4),\n",
       " (642, 4),\n",
       " (631, 4),\n",
       " (904, 4),\n",
       " (673, 4),\n",
       " (560, 4),\n",
       " (577, 4),\n",
       " (579, 4),\n",
       " (500, 4),\n",
       " (721, 4),\n",
       " (896, 4),\n",
       " (571, 4),\n",
       " (540, 4),\n",
       " (570, 4),\n",
       " (616, 4),\n",
       " (534, 4),\n",
       " (469, 4),\n",
       " (576, 4),\n",
       " (583, 4),\n",
       " (565, 4),\n",
       " (648, 4),\n",
       " (665, 4),\n",
       " (510, 4),\n",
       " (690, 4),\n",
       " (621, 4),\n",
       " (795, 4),\n",
       " (728, 4),\n",
       " (517, 4),\n",
       " (572, 4),\n",
       " (543, 4),\n",
       " (595, 4),\n",
       " (587, 4),\n",
       " (514, 4),\n",
       " (635, 4),\n",
       " (760, 4),\n",
       " (889, 4),\n",
       " (533, 4),\n",
       " (654, 4),\n",
       " (660, 4),\n",
       " (656, 4),\n",
       " (794, 4),\n",
       " (680, 4),\n",
       " (528, 4),\n",
       " (809, 4),\n",
       " (502, 4),\n",
       " (573, 4),\n",
       " (529, 4),\n",
       " (614, 4),\n",
       " (740, 3),\n",
       " (898, 3),\n",
       " (901, 3),\n",
       " (626, 3),\n",
       " (481, 3),\n",
       " (435, 3),\n",
       " (663, 3),\n",
       " (520, 3),\n",
       " (687, 3),\n",
       " (859, 3),\n",
       " (601, 3),\n",
       " (979, 3),\n",
       " (782, 3),\n",
       " (591, 3),\n",
       " (730, 3),\n",
       " (812, 3),\n",
       " (679, 3),\n",
       " (623, 3),\n",
       " (700, 3),\n",
       " (691, 3),\n",
       " (581, 3),\n",
       " (1094, 3),\n",
       " (757, 3),\n",
       " (536, 3),\n",
       " (454, 3),\n",
       " (683, 3),\n",
       " (745, 3),\n",
       " (484, 3),\n",
       " (793, 3),\n",
       " (783, 3),\n",
       " (462, 3),\n",
       " (825, 3),\n",
       " (758, 3),\n",
       " (1076, 3),\n",
       " (719, 3),\n",
       " (491, 3),\n",
       " (806, 3),\n",
       " (820, 3),\n",
       " (549, 3),\n",
       " (651, 3),\n",
       " (766, 3),\n",
       " (464, 3),\n",
       " (596, 3),\n",
       " (602, 3),\n",
       " (829, 3),\n",
       " (938, 3),\n",
       " (643, 3),\n",
       " (544, 3),\n",
       " (633, 3),\n",
       " (667, 3),\n",
       " (554, 3),\n",
       " (545, 3),\n",
       " (717, 3),\n",
       " (613, 3),\n",
       " (887, 3),\n",
       " (902, 3),\n",
       " (749, 3),\n",
       " (947, 3),\n",
       " (664, 3),\n",
       " (606, 3),\n",
       " (735, 3),\n",
       " (821, 3),\n",
       " (702, 3),\n",
       " (527, 3),\n",
       " (1041, 3),\n",
       " (526, 3),\n",
       " (761, 3),\n",
       " (678, 3),\n",
       " (551, 3),\n",
       " (672, 3),\n",
       " (507, 3),\n",
       " (961, 3),\n",
       " (701, 2),\n",
       " (703, 2),\n",
       " (546, 2),\n",
       " (811, 2),\n",
       " (618, 2),\n",
       " (1140, 2),\n",
       " (640, 2),\n",
       " (496, 2),\n",
       " (921, 2),\n",
       " (696, 2),\n",
       " (630, 2),\n",
       " (622, 2),\n",
       " (1279, 2),\n",
       " (873, 2),\n",
       " (772, 2),\n",
       " (832, 2),\n",
       " (771, 2),\n",
       " (615, 2),\n",
       " (808, 2),\n",
       " (592, 2),\n",
       " (671, 2),\n",
       " (1038, 2),\n",
       " (907, 2),\n",
       " (784, 2),\n",
       " (1116, 2),\n",
       " (650, 2),\n",
       " (870, 2),\n",
       " (519, 2),\n",
       " (695, 2),\n",
       " (559, 2),\n",
       " (557, 2),\n",
       " (607, 2),\n",
       " (714, 2),\n",
       " (715, 2),\n",
       " (858, 2),\n",
       " (521, 2),\n",
       " (790, 2),\n",
       " (636, 2),\n",
       " (742, 2),\n",
       " (639, 2),\n",
       " (1084, 2),\n",
       " (764, 2),\n",
       " (638, 2),\n",
       " (711, 2),\n",
       " (774, 2),\n",
       " (675, 2),\n",
       " (652, 2),\n",
       " (1001, 2),\n",
       " (756, 2),\n",
       " (1438, 2),\n",
       " (913, 2),\n",
       " (770, 2),\n",
       " (851, 2),\n",
       " (612, 2),\n",
       " (1189, 2),\n",
       " (704, 2),\n",
       " (911, 2),\n",
       " (973, 2),\n",
       " (814, 2),\n",
       " (1162, 2),\n",
       " (486, 2),\n",
       " (710, 2),\n",
       " (608, 2),\n",
       " (649, 2),\n",
       " (724, 2),\n",
       " (826, 2),\n",
       " (566, 2),\n",
       " (797, 2),\n",
       " (914, 2),\n",
       " (659, 2),\n",
       " (658, 2),\n",
       " (645, 2),\n",
       " (590, 2),\n",
       " (751, 2),\n",
       " (578, 2),\n",
       " (1198, 2),\n",
       " (828, 2),\n",
       " (748, 2),\n",
       " (733, 2),\n",
       " (689, 2),\n",
       " (1042, 2),\n",
       " (575, 2),\n",
       " (670, 2),\n",
       " (753, 2),\n",
       " (609, 2),\n",
       " (594, 2),\n",
       " (1726, 2),\n",
       " (726, 2),\n",
       " (827, 2),\n",
       " (1297, 2),\n",
       " (692, 2),\n",
       " (662, 2),\n",
       " (969, 2),\n",
       " (1230, 2),\n",
       " (512, 2),\n",
       " (693, 2),\n",
       " (593, 2),\n",
       " (697, 2),\n",
       " (747, 2),\n",
       " (746, 2),\n",
       " (830, 2),\n",
       " (775, 2),\n",
       " (619, 2),\n",
       " (605, 2),\n",
       " (776, 2),\n",
       " (1226, 2),\n",
       " (477, 2),\n",
       " (863, 2),\n",
       " (556, 2),\n",
       " (567, 2),\n",
       " (1067, 2),\n",
       " (1118, 2),\n",
       " (669, 2),\n",
       " (586, 2),\n",
       " (634, 2),\n",
       " (803, 2),\n",
       " (780, 2),\n",
       " (1437, 2),\n",
       " (694, 2),\n",
       " (948, 1),\n",
       " (903, 1),\n",
       " (1233, 1),\n",
       " (2856, 1),\n",
       " (860, 1),\n",
       " (1626, 1),\n",
       " (1091, 1),\n",
       " (732, 1),\n",
       " (2050, 1),\n",
       " (1880, 1),\n",
       " (1476, 1),\n",
       " (861, 1),\n",
       " (1621, 1),\n",
       " (899, 1),\n",
       " (949, 1),\n",
       " (706, 1),\n",
       " (1000, 1),\n",
       " (738, 1),\n",
       " (962, 1),\n",
       " (796, 1),\n",
       " (1568, 1),\n",
       " (759, 1),\n",
       " (929, 1),\n",
       " (866, 1),\n",
       " (2055, 1),\n",
       " (752, 1),\n",
       " (2784, 1),\n",
       " (708, 1),\n",
       " (641, 1),\n",
       " (1254, 1),\n",
       " (2031, 1),\n",
       " (1053, 1),\n",
       " (2570, 1),\n",
       " (1071, 1),\n",
       " (617, 1),\n",
       " (1208, 1),\n",
       " (1572, 1),\n",
       " (2128, 1),\n",
       " (1427, 1),\n",
       " (1819, 1),\n",
       " (1318, 1),\n",
       " (1326, 1),\n",
       " (779, 1),\n",
       " (2130, 1),\n",
       " (1167, 1),\n",
       " (1458, 1),\n",
       " (785, 1),\n",
       " (1212, 1),\n",
       " (1143, 1),\n",
       " (1304, 1),\n",
       " (815, 1),\n",
       " (1037, 1),\n",
       " (1361, 1),\n",
       " (988, 1),\n",
       " (976, 1),\n",
       " (1839, 1),\n",
       " (958, 1),\n",
       " (1586, 1),\n",
       " (624, 1),\n",
       " (1307, 1),\n",
       " (1378, 1),\n",
       " (1543, 1),\n",
       " (1727, 1),\n",
       " (1002, 1),\n",
       " (505, 1),\n",
       " (1625, 1),\n",
       " (1150, 1),\n",
       " (1335, 1),\n",
       " (888, 1),\n",
       " (804, 1),\n",
       " (653, 1),\n",
       " (912, 1),\n",
       " (504, 1),\n",
       " (1064, 1),\n",
       " (1188, 1),\n",
       " (1165, 1),\n",
       " (895, 1),\n",
       " (1341, 1),\n",
       " (1662, 1),\n",
       " (978, 1),\n",
       " (965, 1),\n",
       " (1260, 1),\n",
       " (1311, 1),\n",
       " (1095, 1),\n",
       " (1025, 1),\n",
       " (1407, 1),\n",
       " (1152, 1),\n",
       " (878, 1),\n",
       " (845, 1),\n",
       " (707, 1),\n",
       " (1863, 1),\n",
       " (1408, 1),\n",
       " (1275, 1),\n",
       " (1050, 1),\n",
       " (1287, 1),\n",
       " (773, 1),\n",
       " (805, 1),\n",
       " (849, 1),\n",
       " (1018, 1),\n",
       " (1100, 1),\n",
       " (885, 1),\n",
       " (995, 1),\n",
       " (818, 1),\n",
       " (1019, 1),\n",
       " (1088, 1),\n",
       " (1119, 1),\n",
       " (915, 1),\n",
       " (1962, 1),\n",
       " (807, 1),\n",
       " (1305, 1),\n",
       " (1784, 1),\n",
       " (982, 1),\n",
       " (850, 1),\n",
       " (875, 1),\n",
       " (777, 1),\n",
       " (750, 1),\n",
       " (1134, 1),\n",
       " (537, 1),\n",
       " (1192, 1),\n",
       " (2502, 1),\n",
       " (1014, 1),\n",
       " (769, 1),\n",
       " (1413, 1),\n",
       " (1156, 1),\n",
       " (713, 1),\n",
       " (2366, 1),\n",
       " (629, 1),\n",
       " (1760, 1),\n",
       " (2632, 1),\n",
       " (1010, 1),\n",
       " (1146, 1),\n",
       " (1460, 1),\n",
       " (511, 1),\n",
       " (1129, 1),\n",
       " (1550, 1),\n",
       " (925, 1),\n",
       " (1149, 1),\n",
       " (841, 1),\n",
       " (699, 1),\n",
       " (862, 1),\n",
       " (709, 1),\n",
       " (1108, 1),\n",
       " (1325, 1),\n",
       " (1062, 1),\n",
       " (1068, 1),\n",
       " (909, 1),\n",
       " (1623, 1),\n",
       " (1085, 1),\n",
       " (627, 1),\n",
       " (2038, 1),\n",
       " (1201, 1),\n",
       " (1640, 1),\n",
       " (1548, 1),\n",
       " (957, 1),\n",
       " (1864, 1),\n",
       " (1494, 1),\n",
       " (846, 1),\n",
       " (1082, 1),\n",
       " (991, 1),\n",
       " (1492, 1),\n",
       " (1077, 1),\n",
       " (868, 1),\n",
       " (1328, 1),\n",
       " (880, 1),\n",
       " (854, 1),\n",
       " (2303, 1),\n",
       " (844, 1),\n",
       " (589, 1),\n",
       " (1061, 1),\n",
       " (1887, 1),\n",
       " (810, 1),\n",
       " (1141, 1),\n",
       " (727, 1),\n",
       " (1102, 1),\n",
       " (1031, 1),\n",
       " (763, 1),\n",
       " (1027, 1),\n",
       " (1170, 1),\n",
       " (882, 1),\n",
       " (852, 1),\n",
       " (970, 1),\n",
       " (778, 1),\n",
       " (646, 1),\n",
       " (930, 1),\n",
       " (897, 1),\n",
       " (847, 1),\n",
       " (1008, 1),\n",
       " (1049, 1),\n",
       " (1051, 1),\n",
       " (1557, 1),\n",
       " (741, 1),\n",
       " (1481, 1),\n",
       " (944, 1),\n",
       " (891, 1),\n",
       " (1021, 1),\n",
       " (2067, 1),\n",
       " (743, 1),\n",
       " (1060, 1),\n",
       " (792, 1),\n",
       " (1092, 1),\n",
       " (1069, 1),\n",
       " (883, 1),\n",
       " (1862, 1),\n",
       " (996, 1),\n",
       " (817, 1),\n",
       " (985, 1),\n",
       " (1103, 1),\n",
       " (2866, 1),\n",
       " (718, 1),\n",
       " (840, 1),\n",
       " (2575, 1),\n",
       " (2084, 1),\n",
       " ...]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([len(seq) for _, seq in sequences.items()]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108646/108646 [00:03<00:00, 33581.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_data = Actions()\n",
    "\n",
    "for _, seq in tqdm(sequences.items()):\n",
    "    actions_data.addSequence([s[0] for s in seq])\n",
    "    \n",
    "actions_data.n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2],\n",
       "         [6],\n",
       "         [1]]),\n",
       " tensor([[2],\n",
       "         [6],\n",
       "         [1]]),\n",
       " [0, 1, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "def indexesFromSentence(sequence):\n",
    "    return [(actions_data.action2index[action], tau) for action, tau in sequence]\n",
    "\n",
    "\n",
    "def tensorFromSentence(sequence):\n",
    "    results = indexesFromSentence(sequence)\n",
    "    results.append((EOS_token, 1))\n",
    "    indexes = [i[0] for i in results]\n",
    "    taus = [i[1] for i in results]\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1), taus\n",
    "\n",
    "\n",
    "def tensorsFromPair(action):\n",
    "    input_tensor, tau = tensorFromSentence(action)\n",
    "    #target_tensor = tensorFromSentence(action)\n",
    "    target_tensor = input_tensor[:]\n",
    "    return (input_tensor, target_tensor, tau)\n",
    "\n",
    "tensorsFromPair([('session', 0), ('paid_story_view', 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7311, 0.7311])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(torch.ones(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTasa(nn.Module):\n",
    "    def calc_proba(self, tau, inputs):\n",
    "        #print(self.theta)\n",
    "        #print(self.mu)\n",
    "        #print()\n",
    "        #print(inputs[0].item())\n",
    "        #assert 1 == 2\n",
    "        #ix = actions_data.action2index(inputs)\n",
    "        if len(inputs.shape) == 1:\n",
    "            ix = inputs[0].item()\n",
    "        else:\n",
    "            ix = inputs.item()\n",
    "        #print(self.mu[ix], tau)\n",
    "        #print(F.sigmoid(self.theta[ix] + self.mu[ix] * tau))\n",
    "        return F.sigmoid(self.theta[ix] + self.mu[ix] * tau)\n",
    "\n",
    "\n",
    "class EncoderTASA(RNNTasa):\n",
    "    def __init__(self, actions_count, hidden_size):\n",
    "        super(EncoderTASA, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(actions_count, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        self.theta = nn.Parameter(torch.zeros(actions_count))\n",
    "        self.mu = nn.Parameter(torch.zeros(actions_count))\n",
    "    \n",
    "    def forward(self, inputs, hidden, tau):\n",
    "        embedded = self.embedding(inputs).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        prob = self.calc_proba(tau, inputs)\n",
    "        output *= prob\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class DecoderTASA(RNNTasa):\n",
    "    def __init__(self, hidden_size, actions_count):\n",
    "        super(DecoderTASA, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(actions_count, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.lin = nn.Linear(hidden_size, actions_count)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.theta = nn.Parameter(torch.zeros(actions_count))\n",
    "        self.mu = nn.Parameter(torch.zeros(actions_count))\n",
    "\n",
    "    def forward(self, inputs, hidden, tau):\n",
    "        output = self.embedding(inputs).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        prob = self.calc_proba(tau, inputs)\n",
    "        output *= prob\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.lin(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "MAX_LENGTH = max(len(seq) for _, seq in sequences.items()) + 2\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, taus, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden, taus[ei])\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, taus[di])\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            #print(decoder_output[0].argmax().numpy())\n",
    "            #print(target_tensor[di][0].argmax().numpy())\n",
    "            #assert 1 == 2\n",
    "            accuracy += decoder_output[0].argmax().numpy() == target_tensor[di][0].numpy()\n",
    "            #print(decoder_output[0].argmax().numpy(), target_tensor[di][0].numpy())\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, taus[di])\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            accuracy += decoder_output[0].argmax().numpy() == target_tensor[di][0].numpy()\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    if pd.isna(loss.item() / target_length):\n",
    "        print(loss.item(), target_length)\n",
    "        print(input_tensor, target_tensor, taus)\n",
    "\n",
    "    return loss.item() / target_length, accuracy / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, learning_rate=0.01):\n",
    "    random.seed(0)\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    #training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "    #                  for i in range(n_iters)]\n",
    "    training_pairs = [tensorsFromPair(seq) for _, seq in sequences.items()]\n",
    "    random.shuffle(training_pairs)\n",
    "    criterion = nn.NLLLoss()\n",
    "    epochs = 10\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "\n",
    "        pbar = tqdm(range(len(training_pairs)))\n",
    "\n",
    "        for idx in pbar:\n",
    "            training_pair = training_pairs[idx]\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "            taus = training_pair[2]\n",
    "\n",
    "            loss, acc = train(input_tensor, target_tensor, taus, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "            if idx % 5000 == 0:\n",
    "                print(loss)\n",
    "            running_loss += loss\n",
    "            running_acc += acc\n",
    "\n",
    "            scale_value = 1 / max(idx, 1)\n",
    "            pbar.set_description(\n",
    "                \"Epoch: {}/{}, Loss: {:.4f}, Acc: {:.4f}\".format(\n",
    "                    epoch,\n",
    "                    epochs,\n",
    "                    running_loss * scale_value,\n",
    "                    running_acc * scale_value\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 3.8477, Acc: 0.1411:   0%|          | 5/108646 [00:00<1:30:38, 19.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.329814910888672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 38.3292, Acc: 0.3663:   5%|▍         | 5000/108646 [08:16<2:43:35, 10.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.086479187011719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 36.2314, Acc: 0.3878:   9%|▉         | 10004/108646 [16:21<2:35:35, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.30416488647461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 39.5948, Acc: 0.4017:  14%|█▍        | 15007/108646 [24:31<2:27:57, 10.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.64348856608073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 38.7426, Acc: 0.4182:  18%|█▊        | 20004/108646 [32:44<1:56:13, 12.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.59064178466797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 40.4145, Acc: 0.4232:  23%|██▎       | 25004/108646 [40:55<3:33:24,  6.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.995003836495535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 40.9928, Acc: 0.4268:  28%|██▊       | 30000/108646 [49:16<2:01:02, 10.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1920927533992653e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 42.8623, Acc: 0.4279:  32%|███▏      | 35001/108646 [57:41<3:44:29,  5.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162.8965637703252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 43.7199, Acc: 0.4291:  37%|███▋      | 40002/108646 [1:06:16<2:00:14,  9.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9802313861182483e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 42.7647, Acc: 0.4308:  41%|████▏     | 45003/108646 [1:14:13<1:42:38, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.502217955508474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 42.7956, Acc: 0.4318:  46%|████▌     | 50005/108646 [1:22:41<1:09:04, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.88120727539062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 42.1225, Acc: 0.4353:  51%|█████     | 55000/108646 [1:30:52<1:10:31, 12.68it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.14049784342448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 41.6557, Acc: 0.4369:  55%|█████▌    | 60003/108646 [1:39:17<55:15, 14.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.13900232796718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 42.0969, Acc: 0.4360:  60%|█████▉    | 65002/108646 [1:47:45<1:06:00, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.153201293945315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 41.5223, Acc: 0.4373:  64%|██████▍   | 70001/108646 [1:56:18<54:23, 11.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.16193590666118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 41.0034, Acc: 0.4388:  69%|██████▉   | 75004/108646 [2:04:39<50:24, 11.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.450724283854168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 40.6049, Acc: 0.4401:  74%|███████▎  | 80001/108646 [2:13:02<1:03:09,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.850738245412844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 39.9744, Acc: 0.4424:  78%|███████▊  | 85006/108646 [2:21:37<19:04, 20.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 40.6071, Acc: 0.4440:  83%|████████▎ | 90002/108646 [2:29:41<34:15,  9.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.27512003580729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 40.3611, Acc: 0.4458:  87%|████████▋ | 95000/108646 [2:38:13<20:56, 10.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.689158313679247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 40.0075, Acc: 0.4480:  92%|█████████▏| 100000/108646 [2:46:45<08:16, 17.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.597311019897461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 40.0332, Acc: 0.4493:  97%|█████████▋| 105002/108646 [2:54:47<04:18, 14.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.763690766834078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 39.9923, Acc: 0.4502: 100%|██████████| 108646/108646 [3:00:47<00:00, 10.02it/s]\n",
      "Epoch: 1/10, Loss: 56.6248, Acc: 0.4697:   0%|          | 3/108646 [00:00<1:30:55, 19.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.62242713341345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 42.3161, Acc: 0.4830:   5%|▍         | 5000/108646 [08:44<3:00:26,  9.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 36.3249, Acc: 0.4909:   9%|▉         | 10004/108646 [16:37<2:17:45, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.801849365234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 39.2452, Acc: 0.4856:  14%|█▍        | 15002/108646 [24:32<2:50:11,  9.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.482566833496094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 38.9151, Acc: 0.4836:  18%|█▊        | 20005/108646 [32:49<2:13:38, 11.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.348182678222656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 40.6623, Acc: 0.4807:  23%|██▎       | 25003/108646 [44:03<5:00:14,  4.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.78363037109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 40.2312, Acc: 0.4807:  28%|██▊       | 30000/108646 [58:04<3:06:39,  7.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.061574935913086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 42.9771, Acc: 0.4709:  32%|███▏      | 35001/108646 [1:10:39<4:31:35,  4.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132.91962017276424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 44.0513, Acc: 0.4678:  37%|███▋      | 40002/108646 [1:21:47<2:09:55,  8.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.148048400878906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 43.4100, Acc: 0.4659:  41%|████▏     | 45002/108646 [1:32:15<2:34:39,  6.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.18081096067267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 43.2485, Acc: 0.4646:  46%|████▌     | 50003/108646 [1:41:02<55:25, 17.63it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.524017333984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 42.4693, Acc: 0.4627:  51%|█████     | 55000/108646 [1:48:54<1:06:13, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.80401611328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 42.1906, Acc: 0.4617:  55%|█████▌    | 60004/108646 [1:57:30<48:03, 16.87it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.86615175189394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 42.5186, Acc: 0.4607:  60%|█████▉    | 65001/108646 [2:06:13<1:27:24,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.0573486328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 42.4210, Acc: 0.4607:  64%|██████▍   | 70001/108646 [2:14:42<53:27, 12.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.0745913856908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 41.9748, Acc: 0.4623:  69%|██████▉   | 75003/108646 [2:23:49<54:30, 10.29it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17801652352015176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 41.7110, Acc: 0.4629:  74%|███████▎  | 80000/108646 [2:32:50<26:14, 18.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.873248459002294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: 41.2439, Acc: 0.4648:  78%|███████▊  | 84970/108646 [2:41:15<44:56,  8.78it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-f43aa60a42de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderTASA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-195-89cf2cec3541>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, learning_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtaus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             loss, acc = train(input_tensor, target_tensor, taus, encoder,\n\u001b[0m\u001b[1;32m     26\u001b[0m                          decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-194-b1a03f1f345e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, taus, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         encoder_output, encoder_hidden = encoder(\n\u001b[0m\u001b[1;32m     21\u001b[0m             input_tensor[ei], encoder_hidden, taus[ei])\n\u001b[1;32m     22\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-9a3717cd667e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, hidden, tau)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    735\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    736\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "encoder = EncoderTASA(actions_data.n_actions, hidden_size).to(device)\n",
    "decoder = DecoderTASA(hidden_size, actions_data.n_actions).to(device)\n",
    "\n",
    "encoder, decoder = trainIters(encoder, decoder, 110000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3539202407143635"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(actions_data.action2count.items(), key=lambda x: x[1])[1] / sum(x[1] for x in actions_data.action2count.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.9480e-02, -1.0658e-01, -1.2088e-01,  5.3807e-02,  1.5843e-01,\n",
       "           -9.8912e-02, -3.1942e-02, -1.4115e-01,  5.2331e-02, -2.4757e-01,\n",
       "           -1.2114e-01,  2.6260e-01, -5.8325e-02, -1.3696e-01,  1.3468e-01,\n",
       "           -1.3414e-01, -3.6619e-01, -2.8182e-01, -1.2632e-01,  5.4148e-02,\n",
       "            1.2105e-01, -3.5259e-04,  8.2975e-02,  3.9645e-03, -5.4616e-02,\n",
       "           -1.0495e-01,  3.9120e-01,  2.2627e-01,  1.1876e-01, -7.7026e-02,\n",
       "           -1.4068e-01,  1.1553e-01,  3.4097e-02,  6.7734e-02, -8.5112e-03,\n",
       "            1.7252e-01, -6.1545e-02,  1.9534e-01, -6.5053e-02, -1.3920e-01,\n",
       "           -4.9069e-02, -6.9064e-02,  2.2285e-01,  1.1560e-01,  1.7960e-01,\n",
       "           -1.7477e-01, -3.5469e-02, -3.2298e-01, -1.3649e-01,  1.4075e-02,\n",
       "           -1.1299e-01,  6.6542e-02, -2.2588e-01, -3.8220e-03,  3.2117e-02,\n",
       "           -6.6468e-02, -4.6543e-02, -5.9039e-02,  2.4116e-01, -1.9042e-02,\n",
       "           -1.1231e-01,  6.0434e-02,  1.3858e-01, -1.9344e-01, -2.6559e-01,\n",
       "           -2.2012e-01,  6.5289e-02, -7.4552e-02, -7.0575e-02, -8.6318e-02,\n",
       "           -1.7947e-01,  8.9138e-02,  1.2439e-01,  9.7209e-03,  2.1436e-01,\n",
       "            5.9864e-03,  9.9790e-02, -9.0461e-02, -1.0279e-01,  3.7482e-02,\n",
       "            1.6040e-02, -1.1237e-01,  6.7492e-02, -1.3890e-01, -4.1737e-03,\n",
       "           -1.7775e-01,  1.1508e-01, -8.6244e-02, -7.7783e-02, -2.3856e-01,\n",
       "           -2.3449e-02,  2.1083e-01, -9.1647e-02, -2.0028e-01, -1.7769e-01,\n",
       "           -2.0062e-01,  1.7933e-01,  2.3588e-01, -1.0859e-01,  1.5983e-01]]],\n",
       "        grad_fn=<StackBackward>),\n",
       " tensor([[[-1.9480e-02, -1.0658e-01, -1.2088e-01,  5.3807e-02,  1.5843e-01,\n",
       "           -9.8912e-02, -3.1942e-02, -1.4115e-01,  5.2331e-02, -2.4757e-01,\n",
       "           -1.2114e-01,  2.6260e-01, -5.8325e-02, -1.3696e-01,  1.3468e-01,\n",
       "           -1.3414e-01, -3.6619e-01, -2.8182e-01, -1.2632e-01,  5.4148e-02,\n",
       "            1.2105e-01, -3.5259e-04,  8.2975e-02,  3.9645e-03, -5.4616e-02,\n",
       "           -1.0495e-01,  3.9120e-01,  2.2627e-01,  1.1876e-01, -7.7026e-02,\n",
       "           -1.4068e-01,  1.1553e-01,  3.4097e-02,  6.7734e-02, -8.5112e-03,\n",
       "            1.7252e-01, -6.1545e-02,  1.9534e-01, -6.5053e-02, -1.3920e-01,\n",
       "           -4.9069e-02, -6.9064e-02,  2.2285e-01,  1.1560e-01,  1.7960e-01,\n",
       "           -1.7477e-01, -3.5469e-02, -3.2298e-01, -1.3649e-01,  1.4075e-02,\n",
       "           -1.1299e-01,  6.6542e-02, -2.2588e-01, -3.8220e-03,  3.2117e-02,\n",
       "           -6.6468e-02, -4.6543e-02, -5.9039e-02,  2.4116e-01, -1.9042e-02,\n",
       "           -1.1231e-01,  6.0434e-02,  1.3858e-01, -1.9344e-01, -2.6559e-01,\n",
       "           -2.2012e-01,  6.5289e-02, -7.4552e-02, -7.0575e-02, -8.6318e-02,\n",
       "           -1.7947e-01,  8.9138e-02,  1.2439e-01,  9.7209e-03,  2.1436e-01,\n",
       "            5.9864e-03,  9.9790e-02, -9.0461e-02, -1.0279e-01,  3.7482e-02,\n",
       "            1.6040e-02, -1.1237e-01,  6.7492e-02, -1.3890e-01, -4.1737e-03,\n",
       "           -1.7775e-01,  1.1508e-01, -8.6244e-02, -7.7783e-02, -2.3856e-01,\n",
       "           -2.3449e-02,  2.1083e-01, -9.1647e-02, -2.0028e-01, -1.7769e-01,\n",
       "           -2.0062e-01,  1.7933e-01,  2.3588e-01, -1.0859e-01,  1.5983e-01]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(torch.tensor([1]), hidden=torch.tensor([[[0.]*100]]), tau=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([ 0.0000e+00,  3.9442e+05,  8.4693e+00, -1.1857e+01,  2.6935e+01,\n",
       "         -1.1951e+02, -4.4665e+04,  2.9790e+00, -7.5319e+04,  1.8509e+00,\n",
       "         -3.7516e+02,  3.9770e+00, -5.4403e+00,  6.3962e+02, -8.0507e+01,\n",
       "         -2.1699e+00,  4.0143e+00,  8.6433e+00,  1.3554e+01, -1.5784e+00,\n",
       "         -1.3280e+01, -1.5167e-01, -9.3447e-02, -2.8740e-01,  2.5208e-06,\n",
       "          1.2130e-26, -1.3107e+06], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0000e+00,  3.9442e+05,  8.4688e+00, -1.1851e+01,  2.6921e+01,\n",
       "         -1.1949e+02, -4.4665e+04,  2.9778e+00, -7.5319e+04,  1.8503e+00,\n",
       "         -3.7515e+02,  3.9766e+00, -5.4400e+00,  6.3961e+02, -8.0505e+01,\n",
       "         -2.1694e+00,  4.0142e+00,  8.6433e+00,  1.3554e+01, -1.5783e+00,\n",
       "         -1.3280e+01, -1.5160e-01, -9.3441e-02, -2.8740e-01,  2.5231e-06,\n",
       "          1.2126e-26, -1.3107e+06], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 5.3291e-02,  9.0535e-02, -6.1772e-01,  ..., -1.6330e+00,\n",
       "          -1.1627e+00, -1.7068e-01],\n",
       "         [-4.1737e+03,  7.6929e+03, -2.1547e+04,  ..., -8.8498e+03,\n",
       "           1.1518e+04, -1.0446e+04],\n",
       "         [ 3.2529e+03,  1.8598e+04, -9.3989e+04,  ..., -5.7229e+04,\n",
       "           5.4345e+04, -1.1877e+04],\n",
       "         ...,\n",
       "         [-6.4250e-01, -7.4425e-01, -2.0659e-01,  ..., -9.5073e-01,\n",
       "           1.9153e+00, -1.0952e+00],\n",
       "         [ 1.9662e+00,  1.0567e+00,  1.8076e+00,  ..., -2.1597e-01,\n",
       "           6.9960e-01, -7.9982e-01],\n",
       "         [ 1.9846e+05,  5.1080e+05, -6.3975e+05,  ..., -3.5452e+05,\n",
       "           5.5355e+05, -8.0275e+05]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0265, -0.1248, -0.2813,  ...,  0.5135, -0.1281,  0.0107],\n",
       "         [ 6.2334,  0.8670,  0.1540,  ..., -1.2477, -8.0539, -1.3930],\n",
       "         [-1.0545,  0.1221, -2.5527,  ...,  0.5666,  0.8136, -2.6026],\n",
       "         ...,\n",
       "         [-2.8452,  0.6799, -7.2033,  ..., -6.3013,  4.2195, -3.9350],\n",
       "         [ 0.1645, -0.4113, -1.8370,  ...,  1.2336, -0.3080, -1.6374],\n",
       "         [-0.5533, -0.6135, -1.7869,  ..., -0.7666,  0.4356, -1.4727]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.6867, -1.0379, -0.5739,  ..., -0.7274, -0.7813,  0.1866],\n",
       "         [ 4.4432,  3.0186,  4.5279,  ...,  4.4081, -0.0695, -4.6416],\n",
       "         [-1.9184, -2.2815, -1.5821,  ..., -1.6315, -0.2577,  0.5818],\n",
       "         ...,\n",
       "         [ 2.4724,  3.7061,  1.6825,  ...,  2.7230,  1.9736,  0.1429],\n",
       "         [-1.9398, -2.3912, -1.3457,  ..., -1.7054, -2.3476,  0.2323],\n",
       "         [-0.4042, -0.4925, -0.1867,  ..., -0.1359, -0.6672,  0.2436]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.1237e+00, -4.1887e+00,  2.5040e+00,  1.5591e-01, -3.8434e+02,\n",
       "         -1.4055e+00,  2.3358e-01,  9.5186e-03,  1.7246e+00, -8.2758e+01,\n",
       "         -1.1743e+00,  3.8582e-01, -3.6278e-01,  2.2373e+01,  4.0478e+00,\n",
       "         -7.6770e-01,  2.4818e+05, -1.1689e+01,  1.6674e+00,  2.2153e+02,\n",
       "         -6.6355e-01,  7.6121e-01, -7.5418e+01,  1.1430e-01, -2.8541e+03,\n",
       "          4.6783e-02,  1.4932e+04, -2.6806e+00,  7.7685e-01,  2.8102e+02,\n",
       "         -7.8295e-01,  2.7291e-01, -3.8313e+01,  1.8432e+00, -1.5105e+00,\n",
       "          6.1462e-01,  8.8377e+00,  2.7584e+00,  5.8698e-01, -4.7936e-01,\n",
       "          1.1195e-01, -7.1804e-01,  5.6814e-01,  8.5149e-01, -1.3627e+00,\n",
       "          1.0024e+00, -1.2293e+04,  3.3310e-01,  3.6496e-01, -3.6030e-01,\n",
       "          6.2077e-01,  2.2828e+00,  3.3558e+00,  9.0293e+00, -1.0884e+02,\n",
       "          1.1045e+00, -4.2405e+00, -6.2022e-01,  1.4279e+03,  5.4568e+00,\n",
       "          4.5628e+02,  6.4768e-01,  2.9300e+00,  1.2145e+00,  2.0200e-01,\n",
       "          6.9655e+03,  1.2004e+02,  1.2047e+00,  2.7322e-01,  2.5797e-01,\n",
       "          4.0703e-01,  1.3007e+00,  4.3885e-01,  4.5150e+02,  6.9610e-01,\n",
       "          3.9114e-01, -3.7998e-02, -6.7800e-01, -1.5417e+00, -8.6984e+00,\n",
       "          4.4771e-01, -7.0053e+04,  8.5165e+00,  4.5013e-01,  1.2629e+02,\n",
       "         -1.6691e+05,  3.5916e-01,  1.3258e+00, -4.0652e+03,  2.6952e-01,\n",
       "          1.7068e+00, -5.7674e+04, -9.0442e-02, -5.5904e+01,  1.2651e+00,\n",
       "         -3.8600e+05,  8.6778e-01,  1.4849e+00, -6.3615e-01, -4.8055e-02,\n",
       "         -5.5147e+00, -7.8928e+03, -4.6349e+00, -3.8957e+00,  2.6098e+05,\n",
       "         -3.8336e+04,  3.2604e+01, -1.1923e+01,  6.4220e+01, -1.0653e+04,\n",
       "          1.8173e+03, -2.6139e+00, -2.1071e+02, -5.6631e+03, -7.2265e+00,\n",
       "         -4.7307e+01, -3.5470e+04,  4.0405e+04, -9.4111e+02, -3.7287e+04,\n",
       "          3.1291e+01,  1.0992e+05,  4.5481e+05,  3.8199e+04,  1.6143e+01,\n",
       "          7.6247e+04, -6.4554e+01, -4.4898e+00,  1.5709e+02, -1.3332e+03,\n",
       "         -3.5229e+00,  1.1560e+05, -1.5277e+00, -3.9462e+00, -9.1634e+00,\n",
       "         -6.4433e+04, -4.0633e+01,  1.6196e+02, -1.9857e+00, -6.4462e+00,\n",
       "          8.9187e+03, -5.8301e+00,  7.8577e+00, -2.6963e+00, -1.3263e+00,\n",
       "         -7.0923e+00, -5.9938e+03,  2.4353e+01,  8.2378e-01,  1.4102e+03,\n",
       "          5.5303e+00, -4.5247e+00,  3.4247e+01, -2.4511e+00,  4.8282e+04,\n",
       "         -4.7059e+01, -1.0598e+02, -3.8480e+03, -9.9166e+00,  1.0277e+01,\n",
       "         -2.4497e+03,  2.3520e+03, -6.6863e+00,  3.5151e+00, -1.0049e+01,\n",
       "          1.3006e+05, -5.1231e+04, -3.5062e+00, -6.0278e+00, -3.3767e+00,\n",
       "         -1.0431e+00, -6.2792e+00, -3.4226e+00, -1.4880e+04, -2.7220e+00,\n",
       "          8.6512e+04,  1.5273e+05,  3.2245e+03,  4.2389e+03, -2.1534e+03,\n",
       "         -6.9729e+04,  1.0202e+02, -6.3805e+00,  2.0380e+04,  1.6775e+05,\n",
       "          1.8622e+04, -6.6092e+00,  8.9420e-01, -1.2271e+05, -9.7121e+01,\n",
       "         -8.2144e-01,  1.5776e+04,  5.9537e+04, -8.9405e+03, -1.8599e+01,\n",
       "         -6.0571e+00,  8.0558e+01,  1.2019e+01,  9.5238e+04,  9.8772e-01,\n",
       "         -1.4899e+01, -3.3571e+00, -1.1711e+01, -1.7814e+00, -3.5191e+02,\n",
       "          7.8915e+00,  2.4736e+00,  3.3498e+01, -1.1788e+01,  5.4968e+02,\n",
       "         -5.6629e+00, -9.2737e+00,  2.3571e+02,  1.1228e+01, -2.3122e+01,\n",
       "          3.0559e+00, -4.1134e+05,  1.9655e+01,  3.5385e+01,  1.8896e+03,\n",
       "         -2.9862e+01, -1.7820e+01,  1.3471e+03, -3.8336e+00,  6.9826e+02,\n",
       "         -3.2724e+00, -8.6296e+04,  1.9483e+01, -1.8311e-01,  2.3676e+01,\n",
       "          4.4445e+03,  4.1597e+00,  7.9036e+02, -7.4641e+00, -2.9154e+01,\n",
       "         -3.4207e+00, -9.3283e+01,  1.2431e+01, -2.6151e+00, -1.0837e+01,\n",
       "         -2.6701e+01, -8.5358e+00, -3.8780e+00, -7.3351e+00,  1.4302e+01,\n",
       "         -6.7624e+01,  1.9787e+04,  5.3438e+01, -4.9093e+00,  3.9045e+00,\n",
       "          2.2196e+01, -1.0653e+01,  1.7299e+01,  1.1573e+01,  1.3184e+02,\n",
       "          4.8334e+00, -1.9743e+01, -3.3734e+00,  5.8246e+04,  5.8727e+01,\n",
       "          9.4421e+02, -1.7080e+00,  2.8450e+01,  9.1930e+00,  2.2974e+01,\n",
       "         -5.6853e+04,  1.0415e+04,  9.3872e+00,  1.2340e+01,  9.6309e-01,\n",
       "         -4.6283e+00,  1.2198e+01,  5.3418e+00,  8.7326e+04, -7.8529e+00,\n",
       "         -9.1556e+00,  1.5810e+00, -2.2328e+00,  9.8248e+00, -1.3113e+01,\n",
       "          3.3941e+00,  2.5281e+05, -6.0365e+01, -1.2240e+00, -1.1562e+03,\n",
       "          1.8084e+05,  1.6220e+01,  1.4269e+01, -3.8869e+04, -1.0187e+01,\n",
       "          1.5408e+01,  7.2585e+04, -6.0238e+00, -3.7865e+02, -1.1548e+01,\n",
       "         -2.6281e+05, -3.6744e+00, -1.1977e+01,  6.0187e+00,  6.9800e-01],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.2923e+00, -4.1922e+00,  2.5537e+00,  1.0442e-01, -3.8423e+02,\n",
       "         -1.4602e+00,  2.6540e-01,  5.2011e-02,  1.8232e+00, -8.2751e+01,\n",
       "         -1.2709e+00,  3.4409e-01, -3.1421e-01,  2.2372e+01,  4.0030e+00,\n",
       "         -6.7536e-01,  2.4818e+05, -1.1611e+01,  1.6571e+00,  2.2154e+02,\n",
       "         -7.1442e-01,  6.0243e-01, -7.5569e+01,  2.2800e-01, -2.8540e+03,\n",
       "          4.6593e-02,  1.4932e+04, -2.6733e+00,  7.4831e-01,  2.8107e+02,\n",
       "         -7.8772e-01,  1.7549e-01, -3.8261e+01,  1.8644e+00, -1.3858e+00,\n",
       "          7.3054e-01,  8.9094e+00,  2.7150e+00,  7.7133e-01, -3.3162e-01,\n",
       "          1.5664e-01, -6.9259e-01,  5.4710e-01,  8.6341e-01, -1.3311e+00,\n",
       "          1.0777e+00, -1.2293e+04,  2.9028e-01,  2.8432e-01, -3.9659e-01,\n",
       "          7.8800e-01,  2.3722e+00,  3.3231e+00,  9.2195e+00, -1.0896e+02,\n",
       "          1.0589e+00, -4.2577e+00, -6.3693e-01,  1.4277e+03,  5.4344e+00,\n",
       "          4.5627e+02,  4.8016e-01,  2.9792e+00,  1.1834e+00,  2.4718e-01,\n",
       "          6.9654e+03,  1.1991e+02,  1.1707e+00,  3.4178e-01,  3.2671e-01,\n",
       "          4.1284e-01,  1.2327e+00,  5.2311e-01,  4.5153e+02,  6.4552e-01,\n",
       "          4.5399e-01,  1.4010e-01, -8.2482e-01, -1.6423e+00, -8.6758e+00,\n",
       "          4.0163e-01, -7.0053e+04,  8.5541e+00,  4.5208e-01,  1.2634e+02,\n",
       "         -1.6691e+05,  2.5484e-01,  1.1896e+00, -4.0653e+03,  3.9503e-01,\n",
       "          1.8696e+00, -5.7674e+04, -1.4146e-01, -5.6038e+01,  1.2326e+00,\n",
       "         -3.8600e+05,  1.0053e+00,  1.5613e+00, -6.9895e-01,  6.8144e-02,\n",
       "         -5.4967e+00, -7.8927e+03, -4.6735e+00, -4.0042e+00,  2.6098e+05,\n",
       "         -3.8336e+04,  3.2565e+01, -1.1851e+01,  6.4173e+01, -1.0653e+04,\n",
       "          1.8172e+03, -2.7045e+00, -2.1070e+02, -5.6632e+03, -7.1751e+00,\n",
       "         -4.7324e+01, -3.5470e+04,  4.0406e+04, -9.4112e+02, -3.7287e+04,\n",
       "          3.1229e+01,  1.0992e+05,  4.5481e+05,  3.8199e+04,  1.6184e+01,\n",
       "          7.6247e+04, -6.4462e+01, -4.4823e+00,  1.5695e+02, -1.3333e+03,\n",
       "         -3.4047e+00,  1.1560e+05, -1.5666e+00, -3.9392e+00, -9.2959e+00,\n",
       "         -6.4433e+04, -4.0627e+01,  1.6207e+02, -2.1366e+00, -6.4786e+00,\n",
       "          8.9186e+03, -5.9796e+00,  7.8395e+00, -2.7970e+00, -1.2401e+00,\n",
       "         -7.0219e+00, -5.9937e+03,  2.4404e+01,  8.8238e-01,  1.4102e+03,\n",
       "          5.4864e+00, -4.5838e+00,  3.4378e+01, -2.4522e+00,  4.8282e+04,\n",
       "         -4.7088e+01, -1.0605e+02, -3.8480e+03, -9.7925e+00,  1.0092e+01,\n",
       "         -2.4496e+03,  2.3520e+03, -6.5906e+00,  3.5748e+00, -1.0093e+01,\n",
       "          1.3006e+05, -5.1231e+04, -3.3891e+00, -5.9960e+00, -3.3136e+00,\n",
       "         -9.2336e-01, -6.2801e+00, -3.4272e+00, -1.4880e+04, -2.7246e+00,\n",
       "          8.6512e+04,  1.5273e+05,  3.2246e+03,  4.2389e+03, -2.1534e+03,\n",
       "         -6.9729e+04,  1.0204e+02, -6.4760e+00,  2.0380e+04,  1.6775e+05,\n",
       "          1.8622e+04, -6.5514e+00,  8.3253e-01, -1.2271e+05, -9.7120e+01,\n",
       "         -8.3562e-01,  1.5776e+04,  5.9537e+04, -8.9405e+03, -1.8716e+01,\n",
       "         -5.9556e+00,  8.0572e+01,  1.1927e+01,  9.5238e+04,  9.0354e-01,\n",
       "         -5.4281e+00, -2.4608e+00, -6.4515e+00, -5.5570e-01, -2.8921e+02,\n",
       "          4.9999e+00,  1.1729e+00,  1.3449e+01, -7.0024e+00,  3.4166e+02,\n",
       "         -1.8709e+00, -4.8395e+00, -7.3112e-01,  4.5388e+00, -1.1596e+01,\n",
       "          1.8249e+00, -2.9989e+05,  1.0727e+01,  2.3462e+01,  7.3806e+02,\n",
       "         -1.4015e+01, -1.8967e+00,  1.3439e+03, -2.2561e+00,  4.7497e+02,\n",
       "         -1.9048e+00, -1.0082e+04,  9.0171e+00,  3.2766e-02, -3.3143e+01,\n",
       "         -3.5869e+00,  2.2472e+00, -1.8517e+00, -3.0136e+00,  1.7727e+00,\n",
       "         -1.3607e+00, -5.4055e+01,  8.6287e+00, -1.4051e+00, -5.0732e+00,\n",
       "         -1.2440e+01, -2.5542e+00, -1.7486e+00, -2.8534e+00,  6.8912e+00,\n",
       "         -7.5112e+01,  1.2012e+04,  2.9173e+01, -2.3008e+00,  1.6034e+00,\n",
       "          1.0722e+01, -5.5246e+00,  9.0816e+00,  1.0758e+01,  1.0206e+02,\n",
       "          3.2139e+00, -9.4491e+00,  1.0434e-01,  5.8201e+04,  2.6094e+01,\n",
       "          4.2314e+02, -5.7663e-01,  1.5002e+01,  4.1599e+00,  7.9775e+00,\n",
       "         -2.1436e+04,  7.2540e+03,  3.8444e+00,  4.8191e+00,  4.3743e-01,\n",
       "         -2.3721e+00,  6.4518e+00,  3.8234e+00,  2.2894e+02, -4.7692e+00,\n",
       "          1.8517e-01,  4.9682e-01, -8.5166e-01,  3.7760e+00, -1.1620e+01,\n",
       "          2.0996e+00,  1.5290e+05, -4.0340e+01,  8.1833e-01, -3.3303e+02,\n",
       "          1.0734e+05,  1.0282e+01,  9.7128e+00, -2.7775e+04, -4.4229e+00,\n",
       "          7.2800e+00,  4.4730e+04, -2.5919e+00, -2.7730e+02, -7.3998e+00,\n",
       "         -1.2631e+05, -1.9490e+00, -6.8396e+00,  3.0456e+00,  1.8789e-01],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p for p in encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
