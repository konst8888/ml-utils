
[34m[1mwandb[39m[22m: [33mWARNING[39m Symlinked 4 files into the W&B run directory, call wandb.save again to sync new files.
Vocab size:  1000
Vocab size:  1000
2.285437
[1448, 8286, 699, 5888, 65188, 2394, 31596, 1198, 595, 158, 23031, 3399, 6016, 30726, 2644, 6337, 1007, 1220, 2263, 2061, 313, 620, 1488, 1228, 2888, 924, 1043, 1123, 1107, 2972, 867, 739, 2736, 816, 427, 1995, 2692]
LR:  0.001














































































































































































































































































































































































































































































Epoch: 0/199 Phase: train Loss: 3.1659 (2.4762) Acc: 0.1107 (0.2200) Prec arts: 0.0805 (0.0000): 100%|â–ˆ| 1720/1720 [15:29<00:00,  1.85it/s]


















































































































































































Epoch: 0/199 Phase: valid Loss: 3.4338 (3.2037) Acc: 0.1582 (0.2000) Prec arts: 0.0664 (nan): 100%|â–ˆâ–ˆâ–ˆâ–‰| 2741/2753 [05:56<00:01,  7.69it/s]
Epoch: 0/199 Phase: valid Loss: 3.4333 (3.7766) Acc: 0.1580 (0.0000) Prec arts: 0.0661 (0.0000): 100%|â–ˆ| 2753/2753 [05:58<00:00,  7.69it/s]

















































































































































































































































































































































































































































































Epoch: 1/199 Phase: train Loss: 2.6403 (1.9902) Acc: 0.3072 (0.5100) Prec arts: 0.1129 (0.2000): 100%|â–ˆ| 1720/1720 [15:32<00:00,  1.84it/s]


















































































































































































Epoch: 1/199 Phase: valid Loss: 2.9734 (3.5092) Acc: 0.2623 (0.2000) Prec arts: 0.0834 (nan): 100%|â–ˆâ–ˆâ–ˆâ–‰| 2746/2753 [05:58<00:00,  7.67it/s]
Epoch: 1/199 Phase: valid Loss: 2.9732 (1.0919) Acc: 0.2622 (0.4286) Prec arts: 0.0832 (nan): 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2753/2753 [05:58<00:00,  7.67it/s]

















































































































































































































































































































































































































































































Epoch: 2/199 Phase: train Loss: 2.0015 (1.9548) Acc: 0.4459 (0.5700) Prec arts: 0.1593 (0.4000): 100%|â–ˆ| 1720/1720 [15:34<00:00,  1.84it/s]


















































































































































































Epoch: 2/199 Phase: valid Loss: 2.9573 (4.4645) Acc: 0.2792 (0.1500) Prec arts: 0.1513 (0.0000): 100%|â–‰| 2741/2753 [05:57<00:01,  7.66it/s]
Epoch: 2/199 Phase: valid Loss: 2.9582 (4.0551) Acc: 0.2792 (0.2857) Prec arts: 0.1513 (nan): 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2753/2753 [05:58<00:00,  7.67it/s]














































































































































































































































































































































































































































































Epoch: 3/199 Phase: train Loss: 1.6817 (1.4719) Acc: 0.5059 (0.5100) Prec arts: 0.2482 (0.0000): 100%|â–ˆ| 1720/1720 [15:26<00:00,  1.86it/s]
























































































































Epoch: 3/199 Phase: valid Loss: 3.0872 (1.7611) Acc: 0.2609 (0.3500) Prec arts: 0.2829 (0.0000):  68%|â–‹| 1865/2753 [04:03<01:56,  7.65it/s]
Traceback (most recent call last):
  File "train.py", line 352, in <module>
    wandb,
  File "train.py", line 90, in train
    for idx, sample in pbar:
  File "/usr/local/lib/python3.6/dist-packages/tqdm/std.py", line 1185, in __iter__
    for obj in iterable:
  File "/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py", line 561, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/root/konst/ml-utils/training/tagger_text/dataset.py", line 169, in __getitem__
    for token in curr_tokens
  File "/root/konst/ml-utils/training/tagger_text/dataset.py", line 169, in <listcomp>
    for token in curr_tokens
