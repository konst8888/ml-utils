
[34m[1mwandb[39m[22m: [33mWARNING[39m Symlinked 4 files into the W&B run directory, call wandb.save again to sync new files.
/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
Epoch: 0/99 Phase: train Loss: 0.3262 (1.0259) Acc: 0.9049 (0.9453) Rec 1: 0.5714 (0.0000):   0%|                    | 6/6250 [00:01<17:12,  6.05it/s]
4.873602
Loaded: 11/11
[793108, 6892]
















































































































































































































































































Epoch: 0/99 Phase: train Loss: 0.2204 (0.1744) Acc: 0.8770 (0.8359) Rec 1: 0.9368 (1.0000):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 3435/6250 [09:07<07:28,  6.28it/s]
Traceback (most recent call last):
  File "train.py", line 326, in <module>
    wandb,
  File "train.py", line 84, in train
    for idx, sample in pbar:
  File "/usr/local/lib/python3.6/dist-packages/tqdm/std.py", line 1185, in __iter__
    for obj in iterable:
  File "/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py", line 561, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/root/konst/ml-utils/training/toxic_deep/dataset.py", line 142, in __getitem__
    for token in curr_tokens
  File "/root/konst/ml-utils/training/toxic_deep/dataset.py", line 142, in <listcomp>
    for token in curr_tokens
KeyboardInterrupt