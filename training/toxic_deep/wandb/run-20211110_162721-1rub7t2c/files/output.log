
[34m[1mwandb[39m[22m: [33mWARNING[39m Symlinked 4 files into the W&B run directory, call wandb.save again to sync new files.
/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
1.009002
[58742, 42039]
LR:  0.001





























































Epoch: 0/99 Phase: train Loss: 0.3435 (0.3270) Acc: 0.8409 (0.8667) Rec 1: 0.7914 (0.6923): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 788/788 [02:05<00:00,  6.29it/s]




















Epoch: 0/99 Phase: valid Loss: 2.2639 (1.8236) Acc: 0.3110 (0.4000) Rec 1: 0.9676 (1.0000):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1240/1260 [00:41<00:00, 30.04it/s]
Epoch: 0/99 Phase: valid Loss: 2.2654 (2.7200) Acc: 0.3109 (0.3125) Rec 1: 0.9665 (0.0000): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [00:41<00:00, 30.08it/s]





























































Epoch: 1/99 Phase: train Loss: 0.2478 (0.1371) Acc: 0.8937 (0.9778) Rec 1: 0.8454 (1.0000): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 788/788 [02:05<00:00,  6.30it/s]




















Epoch: 1/99 Phase: valid Loss: 2.5195 (2.1133) Acc: 0.3035 (0.4000) Rec 1: 0.9657 (1.0000):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1232/1260 [00:41<00:00, 30.25it/s]
Epoch: 1/99 Phase: valid Loss: 2.5178 (2.6245) Acc: 0.3035 (0.1875) Rec 1: 0.9665 (1.0000): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [00:41<00:00, 30.05it/s]





























































Epoch: 2/99 Phase: train Loss: 0.2346 (0.1975) Acc: 0.9006 (0.9333) Rec 1: 0.8576 (0.9000): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 788/788 [02:05<00:00,  6.30it/s]




















Epoch: 2/99 Phase: valid Loss: 2.9137 (2.8430) Acc: 0.2921 (0.4000) Rec 1: 0.9730 (nan):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1218/1260 [00:40<00:01, 30.25it/s]
Epoch: 2/99 Phase: valid Loss: 2.9108 (2.3519) Acc: 0.2922 (0.1875) Rec 1: 0.9738 (1.0000): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [00:41<00:00, 30.23it/s]






























































Epoch: 3/99 Phase: train Loss: 0.2225 (0.2771) Acc: 0.9051 (0.9111) Rec 1: 0.8680 (0.9545): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 788/788 [02:05<00:00,  6.29it/s]



















Epoch: 3/99 Phase: valid Loss: 3.4372 (3.0502) Acc: 0.2747 (0.2500) Rec 1: 0.9804 (nan): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [00:41<00:00, 30.06it/s]
  0%|                                                                                                                 | 0/788 [00:00<?, ?it/s]






























































Epoch: 4/99 Phase: train Loss: 0.2142 (0.1849) Acc: 0.9090 (0.9111) Rec 1: 0.8749 (0.9375): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 788/788 [02:05<00:00,  6.28it/s]




















Epoch: 4/99 Phase: valid Loss: 3.5967 (3.4975) Acc: 0.2812 (0.3000) Rec 1: 0.9788 (1.0000):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1228/1260 [00:40<00:01, 30.29it/s]
Epoch: 4/99 Phase: valid Loss: 3.5973 (3.7329) Acc: 0.2806 (0.3125) Rec 1: 0.9771 (1.0000): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1260/1260 [00:41<00:00, 30.04it/s]
















































Epoch: 5/99 Phase: train Loss: 0.2076 (0.1788) Acc: 0.9129 (0.9297) Rec 1: 0.8797 (0.8750):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 624/788 [01:39<00:26,  6.29it/s]
Traceback (most recent call last):
  File "train.py", line 327, in <module>
    wandb,
  File "train.py", line 104, in train
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.6/dist-packages/torch/cuda/memory.py", line 114, in empty_cache
    torch._C._cuda_emptyCache()
KeyboardInterrupt